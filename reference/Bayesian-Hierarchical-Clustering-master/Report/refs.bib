
@inproceedings{heller_bayesian_2005,
	address = {New York, NY, USA},
	series = {{ICML} '05},
	title = {Bayesian {Hierarchical} {Clustering}},
	isbn = {978-1-59593-180-1},
	url = {http://doi.acm.org/10.1145/1102351.1102389},
	doi = {10.1145/1102351.1102389},
	abstract = {We present a novel algorithm for agglomerative hierarchical clustering based on evaluating marginal likelihoods of a probabilistic model. This algorithm has several advantages over traditional distance-based agglomerative clustering algorithms. (1) It defines a probabilistic model of the data which can be used to compute the predictive distribution of a test point and the probability of it belonging to any of the existing clusters in the tree. (2) It uses a model-based criterion to decide on merging clusters rather than an ad-hoc distance metric. (3) Bayesian hypothesis testing is used to decide which merges are advantageous and to output the recommended depth of the tree. (4) The algorithm can be interpreted as a novel fast bottom-up approximate inference method for a Dirichlet process (i.e. countably infinite) mixture model (DPM). It provides a new lower bound on the marginal likelihood of a DPM by summing over exponentially many clusterings of the data in polynomial time. We describe procedures for learning the model hyperpa-rameters, computing the predictive distribution, and extensions to the algorithm. Experimental results on synthetic and real-world data sets demonstrate useful properties of the algorithm.},
	urldate = {2017-04-23},
	booktitle = {Proceedings of the 22Nd {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Heller, Katherine A. and Ghahramani, Zoubin},
	year = {2005},
	pages = {297--304}
}

@article{gionis_clustering_2007,
	title = {Clustering {Aggregation}},
	volume = {1},
	issn = {1556-4681},
	url = {http://doi.acm.org/10.1145/1217299.1217303},
	doi = {10.1145/1217299.1217303},
	abstract = {We consider the following problem: given a set of clusterings, find a single clustering that agrees as much as possible with the input clusterings. This problem, clustering aggregation, appears naturally in various contexts. For example, clustering categorical data is an instance of the clustering aggregation problem; each categorical attribute can be viewed as a clustering of the input rows where rows are grouped together if they take the same value on that attribute. Clustering aggregation can also be used as a metaclustering method to improve the robustness of clustering by combining the output of multiple algorithms. Furthermore, the problem formulation does not require a priori information about the number of clusters; it is naturally determined by the optimization function. In this article, we give a formal statement of the clustering aggregation problem, and we propose a number of algorithms. Our algorithms make use of the connection between clustering aggregation and the problem of correlation clustering. Although the problems we consider are NP-hard, for several of our methods, we provide theoretical guarantees on the quality of the solutions. Our work provides the best deterministic approximation algorithm for the variation of the correlation clustering problem we consider. We also show how sampling can be used to scale the algorithms for large datasets. We give an extensive empirical evaluation demonstrating the usefulness of the problem and of the solutions.},
	number = {1},
	urldate = {2017-05-01},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Gionis, Aristides and Mannila, Heikki and Tsaparas, Panayiotis},
	month = mar,
	year = {2007},
	keywords = {clustering aggregation, clustering categorical data, correlation clustering, Data clustering}
}

@article{chang2004robust,
  title={Robust path-based clustering for the unsupervised and semi-supervised learning settings},
  author={Chang, Hong and Yeung, Dit-Yan},
  year={2004},
  publisher={Citeseer}
}

@incollection{charytanowicz_complete_2010,
	title = {Complete {Gradient} {Clustering} {Algorithm} for {Features} {Analysis} of {X}-{Ray} {Images}},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-13105-9_2},
	abstract = {Methods based on kernel density estimation have been successfully applied for various data mining tasks. Their natural interpretation together with suitable properties make them an attractive tool among others in clustering problems. In this paper, the Complete Gradient Clustering Algorithm has been used to investigate a real data set of grains. The wheat varieties, Kama, Rosa and Canadian, characterized by measurements of main grain geometric features obtained by X-ray technique, have been analyzed. The proposed algorithm is expected to be an effective tool for recognizing wheat varieties. A comparison between the clustering results obtained from this method and the classical k-means clustering algorithm shows positive practical features of the Complete Gradient Clustering Algorithm.},
	language = {en},
	urldate = {2017-05-01},
	booktitle = {Information {Technologies} in {Biomedicine}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Charytanowicz, Małgorzata and Niewczas, Jerzy and Kulczycki, Piotr and Kowalski, Piotr A. and Łukasik, Szymon and Żak, Sławomir},
	year = {2010},
	note = {DOI: 10.1007/978-3-642-13105-9\_2},
	pages = {15--24},
	file = {Snapshot:/Users/Hazel/Library/Application Support/Zotero/Profiles/u859hmbi.default/zotero/storage/ETA2C9JI/978-3-642-13105-9_2.html:text/html}
}
